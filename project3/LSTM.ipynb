{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "from keras.models import model_from_yaml\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 61 unique characters in the list of poems\n"
     ]
    }
   ],
   "source": [
    "# used help from https://machinelearningmastery.com/develop-character-based-neural-language-model-keras/ and \n",
    "# and https://github.com/vivshaw/shakespeare-LSTM\n",
    "\n",
    "with open(\"data/shakespeare_LSTM.txt\") as file:\n",
    "    poems = file.read()\n",
    "\n",
    "# Encoding and decoding\n",
    "chars = sorted(list(set(poems)))\n",
    "num_chars = len(chars)\n",
    "encoding = {c: i for i, c in enumerate(chars)}\n",
    "decoding = {i: c for i, c in enumerate(chars)}\n",
    "print(\"There are {0} unique characters in the list of poems\".format(num_chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of X input data is 94394\n"
     ]
    }
   ],
   "source": [
    "sentence_length = 50\n",
    "skip = 1\n",
    "X_data = []\n",
    "y_data = []\n",
    "for i in range (0, len(poems) - sentence_length, skip): \n",
    "    sentence = poems[i:i + sentence_length]\n",
    "    next_char = poems[i + sentence_length]\n",
    "    X_data.append([encoding[char] for char in sentence])\n",
    "    y_data.append(encoding[next_char])\n",
    "\n",
    "num_sentences = len(X_data)\n",
    "print('Dimension of X input data is {0}'.format(num_sentences))\n",
    "\n",
    "X = np.zeros((num_sentences, sentence_length, num_chars), dtype=np.bool)\n",
    "y = np.zeros((num_sentences, num_chars), dtype=np.bool)\n",
    "for i, sentence in enumerate(X_data):\n",
    "    for t, encoded_char in enumerate(sentence):\n",
    "        X[i, t, encoded_char] = 1\n",
    "    y[i, y_data[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded a corpus of 94444 characters\n",
      "Our corpus contains 61 unique characters.\n",
      "Sliced our corpus into 94394 sentences of length 50\n",
      "Vectorizing X and y...\n",
      "Sanity check y. Dimension: (94394, 61) # Sentences: 94394 Characters in corpus: 61\n",
      "Sanity check X. Dimension: (94394, 50, 61) Sentence length: 50\n",
      "Let's build a brain!\n",
      "Epoch 1/30\n",
      "738/738 [==============================] - 51s 68ms/step - loss: 2.9202\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.60381, saving model to weights-01-2.604.hdf5\n",
      "Epoch 2/30\n",
      "738/738 [==============================] - 50s 68ms/step - loss: 2.1771\n",
      "\n",
      "Epoch 00002: loss improved from 2.60381 to 2.12124, saving model to weights-02-2.121.hdf5\n",
      "Epoch 3/30\n",
      "738/738 [==============================] - 50s 67ms/step - loss: 1.9961\n",
      "\n",
      "Epoch 00003: loss improved from 2.12124 to 1.97303, saving model to weights-03-1.973.hdf5\n",
      "Epoch 4/30\n",
      "738/738 [==============================] - 50s 67ms/step - loss: 1.8878\n",
      "\n",
      "Epoch 00004: loss improved from 1.97303 to 1.87773, saving model to weights-04-1.878.hdf5\n",
      "Epoch 5/30\n",
      "738/738 [==============================] - 50s 68ms/step - loss: 1.8123\n",
      "\n",
      "Epoch 00005: loss improved from 1.87773 to 1.80490, saving model to weights-05-1.805.hdf5\n",
      "Epoch 6/30\n",
      "738/738 [==============================] - 50s 67ms/step - loss: 1.7602\n",
      "\n",
      "Epoch 00006: loss improved from 1.80490 to 1.74698, saving model to weights-06-1.747.hdf5\n",
      "Epoch 7/30\n",
      "738/738 [==============================] - 50s 67ms/step - loss: 1.7052\n",
      "\n",
      "Epoch 00007: loss improved from 1.74698 to 1.69771, saving model to weights-07-1.698.hdf5\n",
      "Epoch 8/30\n",
      "738/738 [==============================] - 50s 67ms/step - loss: 1.6553\n",
      "\n",
      "Epoch 00008: loss improved from 1.69771 to 1.65314, saving model to weights-08-1.653.hdf5\n",
      "Epoch 9/30\n",
      "738/738 [==============================] - 50s 67ms/step - loss: 1.6179\n",
      "\n",
      "Epoch 00009: loss improved from 1.65314 to 1.61384, saving model to weights-09-1.614.hdf5\n",
      "Epoch 10/30\n",
      "738/738 [==============================] - 50s 67ms/step - loss: 1.5810\n",
      "\n",
      "Epoch 00010: loss improved from 1.61384 to 1.57624, saving model to weights-10-1.576.hdf5\n",
      "Epoch 11/30\n",
      "738/738 [==============================] - 50s 67ms/step - loss: 1.5384\n",
      "\n",
      "Epoch 00011: loss improved from 1.57624 to 1.54010, saving model to weights-11-1.540.hdf5\n",
      "Epoch 12/30\n",
      "738/738 [==============================] - 50s 67ms/step - loss: 1.4950\n",
      "\n",
      "Epoch 00012: loss improved from 1.54010 to 1.50610, saving model to weights-12-1.506.hdf5\n",
      "Epoch 13/30\n",
      "738/738 [==============================] - 50s 67ms/step - loss: 1.4678\n",
      "\n",
      "Epoch 00013: loss improved from 1.50610 to 1.49129, saving model to weights-13-1.491.hdf5\n",
      "Epoch 14/30\n",
      "738/738 [==============================] - 50s 67ms/step - loss: 1.4517\n",
      "\n",
      "Epoch 00014: loss improved from 1.49129 to 1.44828, saving model to weights-14-1.448.hdf5\n",
      "Epoch 15/30\n",
      "738/738 [==============================] - 50s 68ms/step - loss: 1.4084\n",
      "\n",
      "Epoch 00015: loss improved from 1.44828 to 1.41207, saving model to weights-15-1.412.hdf5\n",
      "Epoch 16/30\n",
      "738/738 [==============================] - 50s 67ms/step - loss: 1.3767\n",
      "\n",
      "Epoch 00016: loss improved from 1.41207 to 1.38561, saving model to weights-16-1.386.hdf5\n",
      "Epoch 17/30\n",
      "738/738 [==============================] - 50s 67ms/step - loss: 1.3450\n",
      "\n",
      "Epoch 00017: loss improved from 1.38561 to 1.35817, saving model to weights-17-1.358.hdf5\n",
      "Epoch 18/30\n",
      "738/738 [==============================] - 50s 67ms/step - loss: 1.3100\n",
      "\n",
      "Epoch 00018: loss improved from 1.35817 to 1.32875, saving model to weights-18-1.329.hdf5\n",
      "Epoch 19/30\n",
      "738/738 [==============================] - 50s 67ms/step - loss: 1.2862\n",
      "\n",
      "Epoch 00019: loss improved from 1.32875 to 1.29883, saving model to weights-19-1.299.hdf5\n",
      "Epoch 20/30\n",
      "738/738 [==============================] - 50s 67ms/step - loss: 1.2544\n",
      "\n",
      "Epoch 00020: loss improved from 1.29883 to 1.26852, saving model to weights-20-1.269.hdf5\n",
      "Epoch 21/30\n",
      "738/738 [==============================] - 50s 67ms/step - loss: 1.2204\n",
      "\n",
      "Epoch 00021: loss improved from 1.26852 to 1.23781, saving model to weights-21-1.238.hdf5\n",
      "Epoch 22/30\n",
      "738/738 [==============================] - 50s 67ms/step - loss: 1.1885\n",
      "\n",
      "Epoch 00022: loss improved from 1.23781 to 1.20485, saving model to weights-22-1.205.hdf5\n",
      "Epoch 23/30\n",
      "738/738 [==============================] - 50s 67ms/step - loss: 1.1616\n",
      "\n",
      "Epoch 00023: loss improved from 1.20485 to 1.17332, saving model to weights-23-1.173.hdf5\n",
      "Epoch 24/30\n",
      "738/738 [==============================] - 50s 68ms/step - loss: 1.1264\n",
      "\n",
      "Epoch 00024: loss improved from 1.17332 to 1.13925, saving model to weights-24-1.139.hdf5\n",
      "Epoch 25/30\n",
      "738/738 [==============================] - 50s 67ms/step - loss: 1.0904\n",
      "\n",
      "Epoch 00025: loss improved from 1.13925 to 1.10598, saving model to weights-25-1.106.hdf5\n",
      "Epoch 26/30\n",
      "738/738 [==============================] - 50s 67ms/step - loss: 1.0604\n",
      "\n",
      "Epoch 00026: loss improved from 1.10598 to 1.07345, saving model to weights-26-1.073.hdf5\n",
      "Epoch 27/30\n",
      "738/738 [==============================] - 50s 67ms/step - loss: 1.0196\n",
      "\n",
      "Epoch 00027: loss improved from 1.07345 to 1.03959, saving model to weights-27-1.040.hdf5\n",
      "Epoch 28/30\n",
      "738/738 [==============================] - 50s 67ms/step - loss: 0.9884\n",
      "\n",
      "Epoch 00028: loss improved from 1.03959 to 1.00949, saving model to weights-28-1.009.hdf5\n",
      "Epoch 29/30\n",
      "738/738 [==============================] - 50s 67ms/step - loss: 0.9481\n",
      "\n",
      "Epoch 00029: loss improved from 1.00949 to 0.97612, saving model to weights-29-0.976.hdf5\n",
      "Epoch 30/30\n",
      "738/738 [==============================] - 50s 67ms/step - loss: 0.9229\n",
      "\n",
      "Epoch 00030: loss improved from 0.97612 to 0.94487, saving model to weights-30-0.945.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa19d8bf730>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(sentence_length, num_chars)))\n",
    "model.add(Dense(num_chars))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "architecture = model.to_yaml()\n",
    "with open('model.yaml', 'a') as model_file:\n",
    "    model_file.write(architecture)\n",
    "\n",
    "# Set up checkpoints\n",
    "file_path=\"weights-{epoch:02d}-{loss:.3f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor=\"loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "model.fit(X, y, batch_size=128, epochs=30, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_seed(seed):\n",
    "    # Grab a chunk of three words\n",
    "    word_list = seed.split()\n",
    "    i = randint(1, len(word_list) - 3)\n",
    "\n",
    "    bad_start_end = set(['on', 'of', 'from', \"I\", \"O!\", \"and\", \"be\", 'or', 'the', 'than', 'with', 'by'])\n",
    "    bad_start = set(['of'])\n",
    "    bad_end = set(['no', 'an', 'if'])\n",
    "\n",
    "    words = []\n",
    "    for i, word in enumerate(word_list[i:i + 3]):\n",
    "        if not word == \"I\" and not word == \"O!\":\n",
    "            word = word.strip(\"',.;-!:?\").lower()\n",
    "        if i == 0 and word not in bad_start_end | bad_start:\n",
    "            words.append(word)\n",
    "        if i == 1:\n",
    "            words.append(word)\n",
    "        if i == 2 and word not in bad_start_end | bad_end:\n",
    "            words.append(word)\n",
    "\n",
    "    tag = \" \".join(words)\n",
    "    return tag\n",
    "\n",
    "def format_sonnet(text):\n",
    "    formatted = text.split(\"\\n\")\n",
    "\n",
    "    # The first and last line cut off in the middle, so we'll ditch them\n",
    "    formatted = formatted[1:len(formatted) - 1]\n",
    "\n",
    "    # Eliminate empty strings, strings that are just newlines, or other improper strings\n",
    "    formatted = [string for string in formatted if len(string) > 3]\n",
    "\n",
    "    # Put a period on our last string, replacing other punctuation if it's there.\n",
    "    if formatted[-1][-1].isalnum():\n",
    "        formatted[-1] += \".\"\n",
    "    else:\n",
    "        formatted[-1] = formatted[-1][:-1] + \".\"\n",
    "\n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(seed_pattern):\n",
    "    X = np.zeros((1, sentence_length, num_chars), dtype=np.bool)\n",
    "    print(X.shape)\n",
    "    for i, character in enumerate(seed_pattern):\n",
    "        X[0, i, encoding[character]] = 1\n",
    "\n",
    "\n",
    "    generated_text = \"\"\n",
    "    for i in range(650):\n",
    "        prediction = np.argmax(model.predict(X, verbose=0))\n",
    "        generated_text += decoding[prediction]\n",
    "\n",
    "        activations = np.zeros((1, 1, num_chars), dtype=np.bool)\n",
    "        activations[0, 0, prediction] = 1\n",
    "        X = np.concatenate((X[:, 1:, :], activations), axis=1)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "def make_seed(seed_phrase=\"When of the world of live thee all thee,\"):\n",
    "    if seed_phrase:\n",
    "        phrase_length = len(seed_phrase)\n",
    "        pattern = \"\"\n",
    "        for i in range (0, sentence_length):\n",
    "            pattern += seed_phrase[i % phrase_length]\n",
    "    else:\n",
    "        poems_length = len(poems)\n",
    "        seed = randint(0, poems_length - sentence_length)\n",
    "        pattern = poems[seed:seed + sentence_length]\n",
    "\n",
    "    return pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When of the world of live thee all thee,When of th\n",
      "------------- Generated Poem ----------\n",
      "(1, 50, 61)\n",
      "ee.\n",
      "\n",
      "If the dear received then do mine eyes,\n",
      "The earth of thine and thine and thine of thee,\n",
      "Who lov'st thou thou shouldst thou shalt strangets,\n",
      "  The pays the brave of strange all the will,\n",
      "  That in the brave the black as infored lies.\n",
      "\n",
      "Those survey in thee, and then by delight\n",
      "To me a says of thee that I do belovest,\n",
      "When thou art the store to make the winter's part,\n",
      "  And then be that summer's love in lovely grow.\n",
      "\n",
      "The ear the world my mind in heaven still,\n",
      "The endered meretion of the restor,\n",
      "When sought the store to come not fair\n",
      "That my great reported than my love strange:\n",
      "Then who fair a seeth the still decease,\n",
      "When your beauty shall \n"
     ]
    }
   ],
   "source": [
    "\n",
    "seed_tag = make_seed()\n",
    "print(seed_tag)\n",
    "print('------------- Generated Poem ----------')\n",
    "print(generate(seed_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
